# 传统电商和直播电商区别

1、属性对比

传统电商的属性：主要是营销。

直播电商的属性：主要是娱乐+营销，辅以强互动。 

2、商品内容呈现形式对比

传统电商商品内容呈现形式：以图文和详情页的方式为主。

直播电商商品内容呈现形式：通过直播视频的方式，更直观地展示商品。 

3、商业模式对比

传统电商的商业模式：人找货。即用户通过APP、pc端等来搜索商品。

直播电商的商业模式：货找人。

4、商品价格对比

传统电商的商品价格：通常比较稳定，除了大促的时候，商家会跟着搞一些优惠。

直播电商的商品价格：价格比传统电商要低，通常在直播间中可以收到大额的优惠券或者进行一些秒杀活动，价格相对来说比较实惠。

5、转化率对比

传统电商的转化率：较低。因为在价格、商品展示和互动上，传统电商都没有太多优势。

直播电商的转化率：较高。这是因为直播电商有较强的互动性，不仅仅能够通过直播回答用户的一些问题，主播还能通过直播具体展现商品细节，更方便用户看到商品的整体效果。另外，还有大额优惠券和秒杀活动等价格上的优势，因此，转化率相对较高。

6、互动性对比

传统电商的互动性：较弱。这是因为用户只能通过客服来解决问题，而且客服忙碌的话，用户还需要等待一段时间才能有答复。

直播电商的互动性：较高。这是因为不仅能通过直播评论与主播进行实时对话，还可以进行点赞、打赏等与主播互动的行为表达自己的喜欢。同时主播也可以通过红包、抽奖等来提高用户的活跃度，增加与用户的互动。

以上就是传统电商与直播电商的区别。

# 日活跃用户数量DAU下降怎么分析数据解决问题？



- **1、数据准确性判断**

- 不管什么时候拿到一份数据，不要急着进行数据分析，首先判断数据是否准确，特别是相关同事反馈给你的数据，先确认数据的准确性，再进行分析。

- **2、异常数据判断**

- 确认了数据的准确性之后，再来看数据是否真的存在异常情况，有时候只是周期性的变化，单一的数据我们是是看不出具体的情况的，需要结合之前的数据来发现异常情况。

- **3、进行维度拆分**

- 假设活跃度下降了一两万。进行日活跃维度的拆分：

- 按照新老用户的拆分；

- 登入平台的的拆分，比如：IOS、安卓；

- 按照APP版本进行拆分；

- 按照登录渠道的拆分，比如APP、小程序；

- 按照区域的拆分，比如：国家、省份；

- 先计算影响系数，然后每一项数据在跟以往正常的数据做对比，算出影响系数。

- 公式：（今日量 – 昨日量）/（今日总量 – 昨日总量）

- 系数越大，表示影响越大

- **4、业务调整变动**

- 初步确定了原因之后，跟产品、技术、运营人员进行沟通进一步确认原因出现在哪里，主要是沟通最近是否有什么业务上的需求调整。

- 数据分析完重要的是提供建议，先初步确认问题出现在哪，然后与相关人员沟通，根据你掌握的变动因素具体做分析。拿到异常数据要确认数据的准确性和数据是否异常，原始数据不准确分析结果自然也不能作为参考依据。

- # 成交量等项目降低原因

- 基于以上得分析还可以结合漏斗模型，看看哪方面有问题

# SQL的row number

Row_number() 在排名是序号 连续 不重复，即使遇到表中的两个一样的数值亦是如此 行号

rank()Rank() 函数会把要求排序的值相同的归为一组且每组序号一样，排序不会连续执行 间断，不连续

Dense_rank() 排序是连续的，也会把相同的值分为一组且每组排序号一样，连续 重复

# APP指标设定

## 五大维度 用户规模与质量、参与度分析、渠道分析、功能分析、用户属性分析

## 用户规模与质量

（1）活跃用户指标

分为日活、周活、月活等周期内启动过APP的用户。不同类型的APP关注度不同，如用户每天都要打开的APP，新闻、社交、娱乐等，统计日活。低频率打开的如旅游、购物周活。

（2）新增用户指标

主要衡量营销推广渠道效果 的最基础指标，如何新增用户占活跃用户比例很大，那么说明留存率不是很高，活跃主要是通过推广得来

（3）用户构成指标

是对周活、月活用户的构成进行分析，有助于通过新老用户结果了解活跃用户健康度，主要有以下几类：

本周回流客户

连续活跃n周用户

忠诚用户

近期流失用户

（4）用户留存率

用户留存率是指在某一个统计时间端内新增用户中再经过一段时间后仍启动该用应用的用户比例。用户留存率是验证产品吸引用户重要指标，

（5）每个用户总活跃天数

反映用户质量

## 参与度分析

（1）启动次数指标

在某一统计周期内用户启动应用次数

（2）使用时长

总时长、平均时长、单次时长，可以反映量产品活跃度

（3）访问页面

指用户一次启动访问页面数，统计一定周期内应用的访问页面

（4）使用时间间隔

使用时间间隔是指同一用户相邻两次启动的期间间隔。

## 渠道分析

（1）安卓渠道

（2）appstore

## 功能分析

（1）功能活跃指标

关注某功能的活跃人数、新增用户数、用户构成、用户留存

（2）页面访问路径分析

身份：会员非会员、目标、访问路径

（3）页面转化率

电商网站—浏览商品—把商品放入购物车—支付完成

## 用户属性和画像分析

设备终端、运营商、网络、地域、人口统计数据、用户个人兴趣、用户商业兴趣

## 收入分析

总收入、付费人数、付费率、ARPU每用户平均付费，关注转化漏斗的环节

# 设置过程

(1)了解产品形态

了解产品运作逻辑，关注用户角色、信息、渠道以及之间的流转关系

（2）了解业务逻辑

执行产品业务，用户角色需要走过的路径，有什么功能模块。数据之间的流向

（3）业务流程图

将功能模块拆解，分析具体业务流程

（4）将节点化的业务代码化

列出重要的节点，添加统计事件和统计参数

（5）交付开发调整DRD

与产品和开发人员沟通协调，交付数据指标体系

（6）数据分析

# 分析方法

## 产品生命周期分析

初创期、成长期、成熟期、衰退期

（1）初创期



初创期的重点在于验证产品的核心价值，通过我们产品或服务为特定的用户解决问题，需要关注人群画像和留存率、用户规模和质量

目标人群画像验证与我们假设用户群体特征是否一致

留存率 使用时长、频率 等等，验证产品依赖

留存率分为用户类型因素、产品类型因素

用户类型 新用户 老用户

渠道A用户

渠道B用户

产品类型 功能A用户 功能B用户

（2）快速成长期

产品到这里已经有较好的留存率，进入自发增长期，关注新用户的增长、激活、活跃用户整个漏斗分析

（3）成熟期

关注用户生命周期前半段（吸引、激活、留存）往后半段（流失、回流）开始偏移，同时关注商业化转化路径

流失与回流：分析流失用户分群，人群洞察、流失原因分析、定性确定流失原因、根据流失特征反向验证、最终确定原因、策划落实版本

商业转化率：对高质量的用户进行重点运营，低质量的用户通过产品和运营改进手段，使其向高质量用户迁移

# 转化漏斗

其思想是从最终入手，找出每一步用户转化或者流失情况，来监控效果，优化指标实现商业价值

用户获取模型、消费漏斗模型、电商漏斗模型、功能优化漏斗模型

## 用户获取模型

获取用户、激发活跃、提高留存、增加收入、传播

（1）获取用户

付费获取、搜索营销、口碑传播、关注点击率、注册率、注册量、获客成本

（2）激发活跃

界面优化、新手引导、优惠激励等方式激发活跃。关注浏览页面、购物车、提交订单、完成订单漏断转化，指定策略

（3）提高留存

通过push、短信、订阅号、邮件、客服等方式提醒用户继续使用产品，关注留存率、复购率、人均购买次数、召回率

（4）增加收入

关注获客成本、顾客终身价值

（5）传播

播周期的新用户量，邀请转化率，传播系数

# 随机森林

### 为什么样本要放回

如果不放回，每棵树用的样本完全不同，会导致基学习器之间的相似小，错误率高

### 为什么样本要抽样

如果不抽样，基学习器所用样本一样，学习器相关性大，错误率高

### 分类效果

决策树之间相关性越大错误率高

分能力强，错误率低

### 为什么要随机抽样

随机抽样可以产生约1/3样本不会被用于决策树的建立，可以用于袋外数据，用于模型的评价，代替了K折交叉实验

### 特征是否放回

在树与树之间，特征是放回的，但是在决策树的建立过程中，如果是01二分类特征，被用过一次就不会再用，如果是多分类离散变量或者是连续变量，则还是会用

### 特征重要性

通过袋外数据，先计算袋外数据的误差，更改袋外数据中的特征X的值，即加入噪声干扰，计算误差，如果差别很大，则说明X特征非常重要

# ROC AUC

|        | 预测 阳性P | 预测阴性N |
| ------ | ---------- | --------- |
| 阳性 P | TP         | FN        |
| 阴性 N | FP         | TN        |
|        |            |           |

precision(P) =（TP）/（TP+FP）预测为P的集合中预测正确的能力，结果的可信度

recal（P）=（TP）/（TP+FN） 在所有P样本中，预测出P的能力，模型能够检测整个类的比率

p值门槛变低，使得预测为P的值变高，预测为N的值变低，recall变高，precision变低

p值门槛变高，使得预测为p的值贬低，预测为N的值变高，recall变低，precision变高

F1=2*（precision* recall）/（precision+recall）

### ROC

受试者工作特征曲线

X轴，假阳率 FPR=(FP)/(TN+FP) 预测为正实际为负/总负 直观上代表 将负类错分为正例的概率

Y轴，真阳率 TPR=（TP）/（TP+FN）预测为正实际为正/总正.直观上代表能将正例分对的概率

##### ROC绘制

<img src="https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210830152817846.png" alt="image-20210830152817846" style="zoom:67%;" />

接下来从高到低，依次将“score”值作为阀值的thredhold，当测试样本大于或等于这个theredhold时，认为是正样本，否则为负样本。eg：对于第四个样本，其“Socre”值为0.6，那么样本1，2，3，4，均为正，因为其score值均大于0.6， 但其实第三个样本真实值为负，所以此时的FPR概率为0.1 (1/10 误分类为正的样本数/负样本个数，暂且这样理解) 事实上从第3 个样本 FPR就等于0.1了。
得出如下ROC曲线

![image-20210830153015430](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210830153015430.png)

特性：当测试集中的正负样本的分布变化的时候，ROC曲线**能够保持不变**。

### AUC

任取一对正负样本，正样本的score大于负样本的score的概率,

#### 计算方法

1.就是任取一对正样本和负样本，如果正样本概率大于负样本概率，则得1，相同得0.5 小于等于0 然后除以总对数

```python
def AUC(y,pred):
    ans=0
    p_label=0
    n_label=0
    for i in range(len(y)):
        if y[i]==0:
            n_label+=1
            continue
        else:
            p_label+=1
            for j in range(len(y)):
                if y[j]==1:
                    continue
				else:
                    if pred[i]>pred[j]:
                        ans+=1
                    elif pred[i]==pred[j]:
                        ans+=0.5
    return ans/(p_label*n_label)
                            
```

2.计算排名 ，按概率从高到矮降序排序，对于正样本score最高得，排名为rank_n,则比他概率小的有M-1个正样本，rank_n-M得负样本

排名第二高得，排序为rank_n-1,比他小的概率有M-2正样本，rank_n-1-M+1 ＋1是因为有一个正样本在他后面，排名比他高

那么排名最小得正样本，排名为rank_1,比他小得正样本为0，比他小的负样本为rank_1-1

总共有M*N得正负样本对，把正样本大于负样本得概率累加，除以总得样本对，通过方法化简，就可以直接将排名累加，然后减去正样本比正样本高得分数

```python
sum(ranki)-(m*(m+1))/m*n
def AUC(y,pred):
    f=list(zip(pred,label))
    #将按排名真实得标签
    rank=[values2 for values1 ,values2 in sorted(f,key=lambda x:x[0] )]
    ranklist=[i+1 for i in range(len(rank)) if rank[i]==1]
    #取得正样本得排序值
    for i in range(len(label)):
        if label[i]==1:
            p+=1
        else:
            n+=1
    return sum(ranklist)-p*(p+1)/2+...
    
```



### AUC和F1

AUC和F1中都有recall指标，但是AUC还有 FPR F1中有precision约束，AUC总体来说尽量不误报(不能把阴性的变成阳性)，适用于推荐系统，F1适用于不放过任何可能，适用疫情，避免出现漏网之鱼（不能把阳性漏掉测成阴性）

# 逻辑回归

#### 损失函数推导：

设p(y=1|x)=y^,p(y=0|x)=1-y^

p(y|x)=p(y=1|x)^y*p(y=0|x)^(1-y)

使用对数极大似然函数，求得损失函数

![image-20210908224023955](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/img/image-20210908224023955.png)

2.基于交叉熵推到

设y发生得概率为P(y),那么y得信息量为I(y)=-log(p(y)) 信息熵为H(y)=-sum(p(yi)*log(p(yi)))

p是真实分布，q预测分布，希望pq越来越下

那么使用KL相对熵 KL=sum(p(yi)*log(p(yi)/q(yi)) ，得到交叉熵让他们两个得分布  然后再让p(x)=1-p(x)放入

![image-20210908224706688](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/img/image-20210908224706688.png)

#### 损失函数为什么不是均方误差

如果是均方误差，对其求导，损失函数中还是包含sigmod函数，导致参数更新过慢，导致梯度消失

使用交叉熵函数，求导后(y^-y)*x，使损失函数大得更新快，损失函数慢得更新慢

#### sigmoid函数推导

sigmoid函数提出了一个假设，认为不同类别得样本服从得是均值不同，方差相同得高斯分布

即
$$
P(x∣Y=0)∼N(μ0​,σ)
P ( x ∣ Y = 1 ) ∼ N ( μ 1 , σ )
$$
利用联合概率=条件概率*边缘概率

![image-20210905230345751](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/img/image-20210905230345751.png)

# 统计知识

#### 置信区间

根据样本对总体进行参数估计，估计出来的一个误差范围，让总体参数落在这个误差范围的概率，成为置信水平。如果对置信水平越低，可能拒绝实际是正确的理论 1类错误，如果高，接受了错误理论 2类错误。

计算区间

样本的平均值-标准ZX标准误差~样本平均值+标准Zx标准误差

#### 显著性水平

显著性水平是估计总体参数落在某一区间内，可能犯错误的概率为*显著性水平*，用α表示

#### 假设检验

假设检验是对总体参数提出某种假设，然后利用我们样本信息判断假设是否成立的过程

两种假设 H0 原假设 想要拒绝的假设，择备假设想要接受的假设

两种错误 弃真错误 采伪错误

三种检验 左侧：假设关键词不少于，不高于使用右侧，双侧 相等

##### 流程

Z检验 用于总体方差已知，假设均值 或者样本标准差已知S

1.提出假设

2.计算统计量Z

3，确定P值，计算拒绝域

T检验 用于标准差已知，假设均值

卡方检验

未知均值，假设方差

#### 中心极限定理

当样本量足够大时，样本均值的分布慢慢变成正态分布，样本的平均值约等于总体平均值

#### 大数定理

期望：某个事件的期望值，也就是每个结果都是由个字的概率和收益相乘而来，本质就是概率的平均值。

依概率收敛：在实验足够多的情况下，频率就会非常接近概率

##### 切比雪夫大数定理 

X1 X2 XN 相互独立 切实DXi存在有一直上界，则算数平均值依概率收敛于数学期望

##### 伯努利大数定理

X1 X2 XN 相互独立且服从于参数为p的0-分布

频率依概率收敛于统计概率

##### 辛钦大数定理

X1 X2 XN 独立同分布，EXi存在

算数平均值稳定与数学期望

##### 切比雪夫不等式

任意一个数据集 位于其平均是m个标准差范围内的比例，

P{|X-EX|>=ε}<=DX/ε^2

### 总体 

研究对象的全体成为总体

#### 个体

每个元素成为个体

#### 样本

对总体进行抽样，所抽取的部分个体成为样本，

# 模型对异常值的鲁棒性

SVM、KNN、RF、贝叶斯对异常值不敏感，SVM、LR、Kmeans、Adaboost对异常值敏感 bossting对噪声敏感

从损失函数 以及算法流程推到

SVM的决定因素是支持向量机，所以对异常值不敏感，KNN只选取几个比较，RF也是随机抽样 所以不敏感

但是LR Kmeans Adaboost 损失函数中包含距离，adaboost会提高异常值样本的权重，容易过拟合

# 样本不平衡处理

数据重采样 

欠采样：如果样本数量不够的话，这个方法存在局限性，而且随机删除多数样本，可能会删除一些有用的信息

##### 如何调整概率

因为采样，导致模型的先验概率改变，所以需要进行概率调整，设数据平衡后，概率为0.5，此时LR输出y=1的概率为ps 假设平衡前输出的概率为p0和p1 设p(y=1)/p(y=0)=b 那么 p1=2ps*p(y=0) p0=(1-p1)\*p(y=0) 两个式子相处，就可求得p1

![image-20210905233512683](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/img/image-20210905233512683.png)

Tomek links: xi和xj属于不同类别，d(xi,xj)为样本之间的距离，称{xi,xj}为TOMEK 对，如果不存在在Xl 使得xl到xi的距离或者是xj的距离小于d 则其中的一类样本为噪声，或者在边界上，欠采样

过采样：直接复制，没有增加新内容，Smote 生成的少数样本可能改变的数据分布，不能确保生成的数据一定是少数的，而且如果不同类的标签具有相邻的示例，则很难生成准确的数据，处理高维的数据具有局限性， 改进 使用**borderline-SMOTE** 标记小类数据的边界范围，让生成的范围数据落入这个范围 

Msmote 把小数样本分为3不同组，安全样本、边界样本和潜在样本，是基于安全样本进行Knn选择生成少数样本

或者通过Smote +bagging方法，通过booststrap方法从训练集S中抽取K个样本，记为SK，在SK中合成少数样本，是的2类样本的数量相等，以SK为训练集，训练CK模型，通过投票得到T每个样本的类别。

Smote-TomeK links将生成的样本去除噪声

ADASYN：先计算不平衡度 d=ms/ml 计算合成得样本数量，G=(ml-ms)*b  对于每个少数样本用欧式聚类计算K个邻居， L为k个邻居中属于多数样本的样本数目，比例r=l/k 在上一步得到的每一个少数样本ri计算各自的ri =ri/sum(ri),对于每个少数样本的合成数量为ri\*G,然后再k中随机选择一个 合成方法跟smote一致。在多数样本附近采样数多，ADASYN方法不仅可以减少原始不平衡数据分布带来的学习偏差，还可以自适应地将决策边界转移到难以学习的样本上。

算法层面 

修改损失函数 将原本的理论最小误差转化成最小成本代价，因为少数样本和多数样本犯错代价不同，权重不同

代价敏感学习  Adaboost 通过不同的权重来使算法区别对待 随机森林 每次随机抽取一部分小样本，同样抽取一部分同量的大样本，进行训练

XGB LGB可以通过增加少数样本的梯度权重来处理样本不平衡。

模型融合： 比如5000：100，保留100的样，如果选择10个模型，那么随机采样得1000个样本，分成10分，每一份用于模型建立，最终投票决定

新方法：HUSDOS-BOOST 将启发式欠采样结合基于分布式过采样结合adaboost模型

HUS 基于boosting算法 得到样本的采样权重，认为重要的样本不应该被删除，dos生成的样本应该符合数据分布，比如数据特征为性别，源数据的男女比例为15：9，那么生成样本的性别特征比例也应该为15：9 ,连续变量则通过告诉高斯分布进行拟合

# 模型对缺失值的鲁棒性

树模型对缺失值敏感度较低，因为cart树内根据权重进行调整，XGB可以把缺失值都放在左边和右边，比较损失函数

涉及到距离度量的敏感度较高，包括kmeans knn 线性模型 svm

贝叶斯、神经网络对缺失值不敏感

## count(1),count(*),count(列名)

count(1)和count(*)结果是一样的，但是count1忽略所有列，用1代表行，速度更快，不会忽略null

count（列名），忽略字段为null

速度上 如果列名是主键 则count(列名快)，不是的话  count(1)快

# 特征选择

特征选择的目的是在数据集的大量的特征中选择与目标变量有着较强的相关性的变量，在开发预测模型的时候减少输入变量的数量，降低建模成本，以及提高模型精度的过程。

在输入变量主要有2大类型：数值型变量和类别型变量，其中类别型变量可分为布尔类型、序号类型、名词变量

预测模型也可为3种 回归、分类、聚类 

特征选择的方法大致有以下几种：监督学习 ：wrapper filter inner，无监督

filter

如果说模型是回归，输出的数值型类型，变量也是数值型类型，最常见的就是相关系数，如果是线性的就是皮尔逊相关系数(pxy=cov(x,y)/Dx*Dy)  

数值输入、分类输出   cart决策树 IV值

分类输入、分类输出 卡方检验 假设2个变量没有相关性，通过一个实际频率和期望频率，计算二者之间的卡方值，再基于我们提前给出的一个显著性水平和卡方自由度(样本数-限制条件)，如果卡方值大于卡方的阈值，则表示在显著性水平下发生了，则代表原假设错误，可认为有相关性。而卡方分箱则是让数值型变量分成几个箱子，形成离散型变量，再计算

wrapper base 递归消除 

选择一个基础模型，然后通过学习器返回的特征重要性 移除最不重要的，重复，直到符合预期，也可以使用RFECV，通过交叉验证的方式来找到最优特征数量。

inner 模型内部就可以计算哪个特征最重要

树模型

RF输出特征重要性 计算方式：1.得到了模型，因为RF采用随机放回抽样，大约有(1-1/n)^n   总样本数为n 每次被抽到的概率为1/n 不被抽到的概率为1-1/n 那么n次不被抽到的概率为(1-1/n)^n，极限为1/3，1/3样本不会被抽取到，这个样本可以作为袋外样本，计算误差为e1 修改某一个特征，得到误差为e2 如果e1-e2就可以表征为特征重要性，最后将重要性归一化即可。2.计算每个特征的分裂次数 /总分裂次数。3.计算每个特征产生的gini系数

XGBOOST：特征出现次数 平均增益 总增益 lightGBM 次数 总gain 平均增益

降维

# 数据探索

1.了解应用领域并确定目标

2.创建目标数据集

3.数据清洗和预处理：去除噪声和异常值；处理缺失值等

噪声：变量中的随机误差 观测量=真实数据+噪声

异常值是指与大部分的观测变量有明显不同的观测值

噪声处理：分箱处理 pca降维 可以减少数据噪声 正则化 l1 l2 可防止过拟合，而数据噪声的存在会导致我们模型过度拟合噪声，所以可以使用  交叉验证：将数据分成3分 训练 验证 测试 训练集用于训练模型，验证集可以多次使用，用于监测是否发生过拟合，可以调整参数，测试集只能使用一次，评估模型能力 K折交叉验证可以把数据集分成10分 每一次取一份用于测试，然后计算平均误差 也可以将2分用于验证 用于调整参数和监控模型是否过拟合

异常值处理:删除，视为缺失值，KNN平均值修正，不处理

空值处理：均值、众数、最小距离决定，有一些模型内部就可以解决缺失值，比如cart树 xgboost lightgbm 

或者选择对缺失值不敏感的模型贝叶斯等等

4.查看数据是否平衡

4.数据收缩和投影：寻找有用的特征进行表示与目标相关的数据；降维/转换

5.选择合适的数据挖掘任务：分类、回归、聚类

6.数据挖掘算法选择：觉得什么模型和模型评价

7.模型建立

8.解释和可视化：挖掘模式的解释；模型和数据可视化

9.巩固：将经验记录，检查与其他先前提取的知识不一致。

# XGB

预排序、行抽样、列抽样、分裂时梯度选择、调参

### 并行度

XGB并行度不是树颗粒度，而是特征颗粒度，随机森林是树颗粒度

### 预排序

为什么需要预排序 因为在寻找树的最佳分裂点的时候，因为是2叉树，需要将数据分成2分，计算信息增益或者信息增益比或者gini系数，如果不对特征进行排序的话，那么每一次遍历特征都需要再遍历特征 找到样本label 来计算信息增益，如果先对特征进行排序，则直接切片就可以了，并且可以作差加速。

预排序算法：提出了一个block结构存储数据，在block中的特征进行排序，而且特征还需要储存样本的索引，这样才可以根据特征的值进行梯度获取，block存的就是排序好的特征以及特征与原样本的映射关系

缓存优化：由于特征是预排序好的，通过特征找到样本的索引访问样本的梯度可能导致内存跳跃式查找，对精确贪心算法效率影响较大，因为贪心算法是遍历所有样本，但是对加权分位数选择分裂点影响不大，xgboost对每个线程分配了连续的缓存空间，预期接下来要去读取的数据，这样就提高了效率

### 采样

Xgboost对数据进行了行列采样，通过对数据的行采样，可以让每个基学习器的样本不一致，防止过拟合，对数据进行列采样也是同类，每个基学习器的学习的特征不相同，降低过拟合，减少计算，同时可以利用袋外数据进行交叉验证

### 特征分裂

分裂是梯度选择：目标是让损失函数降低最快

​	方法：1.贪心算法，即遍历每个样本，遍历每个特征，找到最佳分裂点，因为特征已经预排序好了，

​				2.近似算法：有两种策略，Globa 学习每颗树前就提出候选切分点，并在每次分裂都采用这种分割 local 每次分裂重新提出候选切分点

​								不加权方式：根据特征k的分布来确定l个候选切分点，将特征分成sk1...skl，然后根据这些候选切分点相应的样本放入对应的桶中，每个桶 GH进行累加，然后在候选切分点集合进行贪心查找。

​								加权方式：之所选择hi作为权重是因为在损失函数中是 hi对loss有加权作用的，也就是在我们不加劝的时候，认为每个样本权重是1，现在是针对每个样本使用hi进行加权，

​						![image-20210904220829340](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/img/image-20210904220829340.png)

如 如果采用不加劝 3等分 这是 (1 1 3 ) (4 5 12 )(45 50 99) 采用加权 则是(1 1 3 4 5 12 ) (45 50 )(99)

### 缺失值处理

缺失值处理：把缺失样本全部放在左子树 和右子树，计算损失函数减少值

### 其他优势

损失函数中自带正则化 叶子结点个数和权重L2

### 调参

max_depth3-10  learning_rate0.1 n_estimatores550 subsample colsample_bytree 0.5-1对行列进行采样 正则化1参数  剪枝 scale_pos_weight 处理样本不平衡 

gamma0.1 0.2  损失函数下降最小值  min_child_weiight最小叶子节点样本权重和1 2 3 4 5 6  eval_metric 评价指标  objective 损失函数类型

通过gridsearchcv GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=5, verbose=1, n_jobs=4)

### 与GBDT优势

支持抽样

预排序并行

正则化

内置交叉验证

# LGM

预排序 直方图 行抽样 列抽样 分裂时梯度选择 特征交叉合并（RBF）、调参

### 预排序 与分桶

针对数值型的特征，先要进行预排序降序，但是目的不是为了后续的寻找最优分裂点，而是分桶构建直方图，分桶的过程有2个参数 一个是max_bin min_data_in_bin  如果一个特征的唯一值个数小于max_bin 则可以直接分桶 只需要控制min_data_in_bin数值符合规定即可，类似于常规的等频率分箱

如果唯一值个数大于max_bin n为样本个数，预期的分箱数为n/min_data_in_bin 然后与max_bin取最小值 ，一般还是选择max_bin 计算mean_bin_size常数 用来定义大数和小数，如果特征值的重复值大于这个数值，则为大数，反之为小数  假设有10000个样本，那么mean=40 假设有5个大数 且大数共有5000个样本，那么mean重新计算(10000-5000)/(250-5)  遍历特征，如果是大数，获得2个切分点上限是自己，下限就是下一个distincvlaue，如果是小数则需要重新计算mean进入下一轮循环进行迭代

离散型则根据离散型的个数和种类进行分桶

### 直方图

通过分桶后得到直方图，桶里面存的是样本个数，一阶梯度之和，二阶梯度之和，这样选择最佳分裂点只需要遍历K次，而且直方图作差加速运算，且支持类别特征，直接分桶

### 抽样

LightGBM对数据进行了行列采样，通过对数据的行采样，可以让每个基学习器的样本不一致，防止过拟合，对数据进行列采样也是同类，每个基学习器的学习的特征不相同，降低过拟合，减少计算，同时可以利用袋外数据进行交叉验证

### 特征分裂

只需要遍历桶的个数就可

### 类别特征处理

one hot可以处理类别特征，但是在树模型不适合，因为如果使用每个节点只能使用是不是，切分产生不平衡，信息增益少，而lightGBM会采取或的关系，X是A或者是C  放入左子节点，反之右

### 单边梯度采样

goss是减少数据量和保证精度的平衡算法，保留大梯度的同时对较小梯度随机采样 让较小的权重*(1-a)/b 

### 互斥特征绑定

先建图，计算特征之间同时为0的个数为权重 ，根据度(出边的个数)进行从大到小排序，对于每个特征，遍历所有的簇。计算矛盾数，如果没有簇或者矛盾数超过阈值，则新建簇，如果矛盾值没有超过阈值，则把特征放入簇中。或者直接按照不为0的个数进行排序，直接分簇。进行合并。合并就是对另一个特征增加一个偏移量。复杂度

### 基于Leaf-wise参数优化

XGB 是lever-wise 会造成不必要的分裂 leaf-wise 但可能过拟合，所以通过max_depth 等参数进行限制

### 参数

max_depth subsample colsample_dbytree 正则化n_estimators learning_rate min_child_weight 通过gridsearch

### 与XGB优势

分桶降低了特征选择的复杂度和速度

单边度采用和互斥特征绑定，降低了数据量

生长方式

支持类别特征的处理

# 激活函数

### sigmoid

函数值域0-1 导数 的值域为0-0.25 

优点：容易计算 

缺点：反向传播容易梯度消失  输出不是0均值

### tanh

值域-1,1 导数值域0,1

优点 输出为0均值 缓解了sigmoid梯度消失，但还是存在

### relu

值域0，z导数 0 或1

计算速度快，解决了梯度消失

# 过拟合欠拟合

### 欠拟合

在训练集和测试集上性能较差，表现为高偏差

#### 解决方法

增加模型复杂度，减少正则化参数，新特征，

### 过拟合

过拟合为训练集表现好，测试集表现差，表现高方差

#### 解决方法

增加样本

去除样本噪音

降低模型复杂度

正则化

# 正则

基于最大后验概率估计

MAP=max(log(p(w|y,x)))=极大似然估计*p(w)

如果w服从拉普拉斯分布，则是l1

如果w服从高斯分布，则是l2

l1将元素进行偏移，并是某些元素为0而产生稀疏性

L2则是将每个元素进行不同比例的放缩

# SVM

SVM选取的损失函数是合页损失函数

软间隔的SVM可以允许模型犯错误，避免过渡拟合数据噪声导致模型过拟合

核函数证明：核函数的内积是否是半正定的  针对非0列矩阵X XT*AX>=0 则A为半正定

![image-20210906110314653](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210906110314653.png)

线性核主要用于线性可分，参数少

高斯核主要用于线性不可分，参数多，通过交叉验证来判别

如果feature很大，样本也很多，则采用LR 或者是linear SVM

如果feature很小，样本一般，采用高斯SVM

如果featur很小，样本很多，则需要手动添加一些feature

和LR区别：

首选损失函数不同，LR是交叉熵损失函数，SVM是合页

模型不同，LR是经验风险最小，SVM是结构风险最小

LR依赖于样本数据，SVM只依赖于样本值支持向量少数数据

推导：

hard SVM 要寻找一个超平面，是得样本点到这个超平面最小距离最大化 样本到超平面(f(x)=sign(WT*X+b))得距离就可以写成  1/||w||\*(wT\*xi+b)  目标函数就是 max(w,b) min(xi,yi) 1/||w||\*(wT\*xi+b) 约束条件 yi(wT\*xi+b)>=0  那么根据这个约束条件，可以设存折一个r>=0 让yi(wT\*xi)=r 通过放缩r=1 修改目标函数为 max(w,b) 1/||w|| 约束条件 yi(wT\*xi+b)>=1 改写成 min(w,b)1/2w\*wT约束条件 yi(wT\*xi+b)>=1 改写成拉格朗日形式， min(w,b)max(lambel)L(w,b,lamebl)=min(w,b)max(lambel)(1/2w\*wT+lambel(1-yi(wi\*Txi+b)))

转化强对偶问题 然后max(lambel)min(w,b)(1/2w\*wT+lambel(1-yi(wi\*Txi+b))) 约束条件 lambel>=0 对w求导和b求导 满足kkt条件

对w求导=0 对b求导=0 lambel>=0 lambel(1-yi(wi\*Txi+b)))=0 yi(wT\*xi+b)>=1

求得只剩下lambel lambel使用SMO 序列最优  固定2个lambeli lambelj 再根据约束添加进行转化，只剩下一个参数，再对目标函数进行求偏导，求得 迭代

如果是软间隔得，那么修改损失函数 可赛=1-yi(wT\*xi+b)，克赛>=0

解决多分类：

##### 1对多

把某个类别归为一类 其他的都是负类  比如 ABCD  A：BCD B:ACD C ABD D:ABC 训练4个模型，然后把测试集丢入四个模型，最大结果即为正确的类

缺点是一旦有新的label进来，需要每个模型进行重新训练

优点：训练模型的个数比较少

##### 1对1

AB AC AD BC BD CD 分成6组，训练6个模型，然后投票

缺点：训练模型个数比较多

优点：有新的label进来，只需要重新训练这个label的标签就好了

#### SVM优点

可以使用核函数可以向低纬向高纬度转化，

只有少数样本是支持向量机样本

### 缺点

传统的SVM不支持多分类

# MLE MAP

MLE 极大似然是认为模型得参数是一个定值，可以通过解方程得思想将其求解，

MAP 是认为模型得参数是源自于某一种潜在得分布，可以对数据观测假设参数服从一个分布

比如投硬币例子，如果10次投都朝上，那么MLE就会直接求方程解的投上得概率为1

但是MAP会提前假设参数是0.5左右

MLE对应得经验风险最小，只侧重训练数据集得损失，而MAP是结构风险最小，再经验风险最小得基础上约束模型得复杂度

MLE=max(p(x|w))

MAP=max(p(w|x))=MLE+p(w)

p(w|x)=p(x|w)*p(w)/p(x)

正则化就是基于MAP推导得，l1假设参数服从拉普拉斯分布，l2服从高斯

# 用概率视角看算法

#### 偏最小二乘

假设数据噪声是服从均值为0 方差为o分布得

那么数据集就是服从 均值为(x*wi，o)得分布

直接基于MLE估计 max(wi)p(y|x,wi) 使用高斯分布

#### 正则化

l1 l2 假设参数服从拉普拉斯 高斯

#### 交叉熵函数

设p(y=1|x)=y^,p(y=0|x)=1-y^

p(y|x)=p(y=1|x)^y*p(y=0|x)^(1-y)

使用对数极大似然函数，求得损失函数

#### sigmoid函数

假设2个类别得样本服从得均值不同，方差相同得高斯分布

![image-20210906115307027](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210906115307027-16311089871451.png)

使用伯努利概率

<img src="https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210906115051807.png" alt="image-20210906115051807" style="zoom:50%;" />

其实是贝叶斯概率 p(c1|x)=(p(x|c1)\*p(c1))/(p(x|c1)\*p(c1)+p(x|c2)*p(c2))

将其分子出下来 就是simoid函数原型了

### EM算法

基于一般的MLE算法，直接求对数似然函数最大即可，但是由于我们的参数Θ服从k个不同参数的高斯分布，所以在似然函数中概率会出现连加的情况，不便于计算。

既然知道了参数服从多个高斯分布，那么就创建一个z分布的隐变量，用于计算，使用概率公式，log(p(x|Θ))=log(p(x,z|Θ))-log(p(z|x,Θ))，我们的目标是让这个式子最大，然后后面的式子同时除以Q（z）分布 ,然后对后面的式子关于Q（z）分布求积分，前面的式子还是等于原式子，后面的积分出来就等于一个ELBO+KL KL肯定是大于等于0的 那么只要让每一次迭代 使得ELBO比上一次迭代大，则就可以达到预期 设Θ argmax(ELBO),将积分形式转化成累加形式，对其化简，然后用拉格朗日函数求导即可求解

# PCA

特征降维一个手段 一个中心 原始特征空间重构，两个基本点 最大投影方差和最小重构距离

### 最大投影方差

假设在u1方向的投影，先对xi进行取中心化，转化投影 aT*b 即是a向量在b向量的投影

求方差最大 约束条件是u1Tu1=1 结果中会出现协方差矩阵 S 引入拉个朗日函数

求得Su1=lambelu1 即u1是S得特征值最大得特征向量

### 最小重构距离

xi 在u1和u2得投影 就可以写成 (xiTu1)u1+(xiTu2)u2 因为投影是aT*b 但是要得到向量还是需要乘以单位向量 

在原P为坐标中，xi=p从1到P (xiTup)up 现在重构为q维度 xi^=k从1到K (xiTuk)uk

重构代价为 xi-xi^ 

![image-20210906141553463](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210906141553463.png)



![image-20210906141619341](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210906141619341.png)

![image-20210906141715673](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210906141715673.png)

# 评分卡流程

熟悉一个业务流程，明确需求，获取数据，特征是学历、年龄、存款、贷款、收入、家庭（老人、孩子、家庭）、消费等特征，进行数据清洗，去除异常值、空值（异常值处理手段、空值处理手段）、样本不平衡处理（手段）、特征分箱（手段）、woe值计算、iv值计算、特征筛选、模型训练、模型上线

### 特征分箱

特征分箱主要分成 监督分箱和非监督

非监督等频率、等距分箱

非监督主要是2个 卡方分箱 bestks分箱

#### 卡方分箱

利用卡方检验原理，假设2列数据没有相关性，计算2列数据的实际频数和期望频数，统计卡方值，然后根据我们给出的自由度和显著性水平，查表得卡方阈值，比较这2个值，如果说计算出得值小于卡方阈值，则表示不能拒绝原假设，则认为原假设是正确的 没有相关性就可以合并了![image-20210906144424212](https://raw.githubusercontent.com/chenyan123456789/-Personal-access-tokens/main/im/image-20210906144424212.png)

#### BestKS

衡量好坏样本累计部分之间得差距，ks越大，则正负区分程度越大

1.计算每个评分区间的好坏账户数

2.统计每个平均累计good/good总 和累计bad总/bad总

3.计算二者差值，得到最大值的ks

将特征从小到大排序，计算最大值的ks 将区间分成2分

迭代 继续分隔，直到符合我们阈值和分箱数，阈值一般设置0.05

##### 另外 KS也可用于风控的模型评估

将门槛或者阈值作为横坐标，纵坐标为TPR FPR ，统计2个数值的曲线，计算各个曲线的最大值，门槛的变大 会导致2者值贬低，门槛边低，2者同时高，但是我们希望TPR 尽可能的高，FPR尽可能低

分箱原则 组内差异尽可能的小，组间差异大，

### woe计算

证据权重，统计每个特征每个分箱后每个箱子的lngoodi/good总-lnbadi/bad总 险客户在所有风险客户的比例和正常客户在所有正常客户的比例的差异，差异越大，则表示对风险各户区分月明显

#### iv计算

将每个特征分箱后sum(goodi/good总-badi/bad总)*woei 如果这个值大于0.3 则是比较好优的 IV值就考虑了这个变量能够区分风险客户的数量

### 群体稳定性

反应验证样本在各特征分箱分布与建模样本的分布的稳定性，可用于特征筛选和评估模型重要性

把验证样本认为是实际，建模为预期

psi=sum((Ai-Ei)*in(Ai-Ei) )<0.1则可以

i为分箱数

###  相对熵

KL(P||Q)=P(X)log(p(x)/Q(x)) 衡量两个随机变量的距离，p(x)真实分布，q(x)观测

一般把验证样本认为的实际，训练为预期分布

# FM算法

FM算法是在业务中，除了单变量特征，还需要组合特征，但是如果样本中没有出现这个组合特征的话，这个组合特征的权重是不好训练的，那么就想着把一个单变量特征embeddingK为的向量vi，只要出现过单维的特征，就可以训练这个向量，那么任意2个组合特征的权重就可以表示为 viT*vj

通过数学变换，可以让参数降低到n

- 梯度的方向是方向导数中取到最大值的方向，梯度的值是方向导数的最大值

# Kmeans算法及优化

目标函数：让所有样本点到自己的聚类中心的距离尽可能的小 可以直接使用欧氏距离

K值得选取：如果对数据和业务有很强得经验，已经知道样本大致要分成几类，

​					 如果对数据不熟悉，可以采取手肘法，取K从2-6等数据，计算我们目标函数，当从某一个点突然平稳了 则这个K就是聚类得样本得数值

​					Gap Statistic  也是选择不同得K值，计算目标函数，直接求E(logDK)-logDK Gap函数物理含义是随机样本得损失和实际样本得损失差值，DK计算是通过在样本区域内随机产生和原始样本一样多得样本，并对这些样本做Kemeas 多次模拟可以求得logDK得期望，

初始簇得选择，因为kmeans对初始簇中心的选择比较敏感，如果采用随机生成的方式，效果不是很好

​	二分Kmeans：将所有的样本作为一个簇放到队列中，从队列选择一个簇，使得目标函数最低，进行一分为二，再进行迭代 直到K值符合要求

​	kmeas++：在样本中任选一个节点作为第一个簇类中心的，在样本点中计算每一个样本到目前已存在聚类中心的最短距离，即为DX 那么下一个被选择作为簇中心的概率是DX /总DX  随机生成一个0到1的数字，判断属于什么区间，即什么区间的编号作为下一个簇类，目标是距离最近的簇中心的距离尽可能的远。

kmeas||是在kmeans基础上，解决kmeans每次只取一个，一次性取K个，重复long N 次，

​	canopy算法:将样本放入一个list中，获取一个先验值 r1 r2 r1>=r2   然后从样本中选择一个节点，计算他到每个簇中心的距离，如果没有簇，则这个节点成为簇，如果距离d小于r1 则代表属于这个簇中心，添加到这个簇中心内，如果距离小于r2，则簇中心换成这个点，如果大于r1，则代表不属于这个簇，则p形成新簇。l

# 损失函数

1.交叉熵 逻辑回归损失函数 -sum(yi*log(y^)+(1-yi)\*(log(1-y^)))

2.多分类损失函数  -sum(yi*ln(si))   softmax 输出 si=(e^zi)/（sum(e^zj） zi为上一个神经元得输出

3.均方根误差  -sum(y-yi)2

4.hinge损失 loss=max(0,1-y(fx))

5.01损失 非凸函数

6.绝对值损失

7.指数损失 loss=exp[-yf(x)]

# 特征共线处理

共线会导致特征估计失真或者难以估计

假设x1和X2是共线得，在算法中是单独考察x1对Y得作用得，然而x1效应会混入x2作用，导致参数估计失真

树模型 3个特征共线，随机选择一个，那么其他两个带来得信息增益肯定就少了，特征就不用了

pca原理就是求特征向量，肯定是不相关得 l1 l2   l1 让特征产生稀疏，然某些参数等于0 

# 美团面试

1.列表和元组的区别

存储空间不一样 空列表默认存储空间为40，空元组默认为24

- 元组不可变，列表可以变
  - 因为创建的时候内存已经固定好了，但是列表没有固定
- 列表地址连续，元组地址可能连续，可能不连续
- 元组访问速度比列表块，元组可以作为字段的key，list不能作为字典的key
  - 因为列表是可变对象，一旦变动，无法找到value
- 都支持多种数据类型、支持索引和切片
- 创建元组比列表更快
  - 元组存储在单个内存块中，因此创建一个新的元组最多只需要一次调用来分配内存，列表需要2块内存，一块用于存储python对象信息，固定的，一块是可变大小数据库

2.字典和集合区别

- 字典底层原理：
  - 使用哈希表实现，解决哈希冲突得方法是开放寻址解决
    - 哈希冲突：总会存在不同的数据经过计算后得到的值相同，这就是哈希冲突。
    - 解决办法：
      - 开放定址法：
        - 线性探测：从冲突单元起，依次判断下一个单元是否为空，当达到最后一个单元，在从表首开始判断，直到找到空闲区域或者探查完全部单元为止
        - 平方探测法：冲突单元加上1^2+2^2...
        - 双散列函数：使用两个散射函数h1,h2,h1以key为自变量，产生一个0至m—l之间的数作为散列地址，h2也以key为自变量，产生一个l至m—1之间的、并和m互素的数(即m不能被该数整除)作为探查序列的地址增量，即步长是确定得，而平方步长不确定。
      - 链地址法：
        - 思路是将哈希值相同的元素构成一个同义词的单链表，比较关键字得对象是否相同，使用于经常删除和插入
      - 再哈希：构造多个哈希函数

- 字典是kv类型，集合是k类型
- 字典在python3.6之后，有序得，即在遍历得时候会返回我们插入得顺序，而集合是无序得
  - 这是因为python3.6在底层加了一个indices 列表，会记录我们插入顺序，index列表得索引值代表在hash表中得位置（哈希值），值代表插入顺序
  - 集合无序是因为用了哈希算法，然后对长度取余，如果遇到哈希冲突再通过哈希冲突解决得方法。

python各个数据结构得操作时间复杂度

- 列表：

  | append                          | 1    |
  | ------------------------------- | ---- |
  | index or index assignment(查询) | 1    |
  | pop                             | 1    |
  | insert                          | n    |
  | in                              | n    |
  | index(values)                   | n    |
  | remove                          | n    |
  | len                             | 1    |

- 链表

  查找的复杂度都是o(n)，但是删除 插入 等操作再查找的前提下是o1

- dict

  查找、改value 都是o1

3.深拷贝和浅拷贝

- 数字和字符串中的内存都指向同一个地址，所以深拷贝和浅拷贝对于他们而言都是无意义的

- 拷贝父对象，不会拷贝对象的内部的子对象。当不想改变被拷贝的值时

  - 如果子对象是可变对象，list dict set  那么对他进行修改，则也会修改

  - 如果子对象不是可变对象，则修改不影响

  - ```
    groups = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
    new_groups = copy.copy(groups)
    print(id(groups), id(new_groups))
    140315092110600 140315092266184
    print(id(groups[0]), id(new_groups[0]))
    140315092110664 140315092110664
    new_groups[0][0] = 100000
    print(new_groups[0])
    print(groups[0])
    >>>a = {1: [1,2,3]}
    >>> b = a.copy()
    >>> a, b
    ({1: [1, 2, 3]}, {1: [1, 2, 3]})
    >>> a[1].append(4)
    >>> a, b
    ({1: [1, 2, 3, 4]}, {1: [1, 2, 3, 4]})
    ```

4. between 是否使用索引

- 针对时间的字段 建立索引，使用between 在查询数据条数约占总条数五分之一以下时能够使用到索引，但超过五分之一时，则使用全表扫描了

  - 索引失效情况：

    - where中存在!=

    - where中存在函数条件

    - 在join时候，只有在主键、外键的数据类型相同才能使用

    - where中查询条件使用了比较符LIKE，REGEXP mysql只有在搜索模板的第一个字符的情况下能使用，即以什么开头，LIKE“abc%”使用索引，LIKE“%abc”则不适应

    - order by 操作时候只有排序条件不是查询条件才能使用索引，

    - 如果某个数据列包含许多重复值，则不需要建立，因为效果不好

    - 带头大哥不能死，中间兄弟不能断；

    - | where a=3                                   | Y,使用到a                              |
      | ------------------------------------------- | -------------------------------------- |
      | where a=3 and b=5                           | Y,使用到a,b                            |
      | where a=3 and b=5 and c=4                   | Y,使用到a,b,c                          |
      | where b=3 或 where c=4 或 where b=3 and c=4 | N                                      |
      | **where a=3 and c=5**                       | **Y,使用到a,但是c不可以，b中间断了**   |
      | where a=3 and b>4 and c=5                   | Y,使用到a和b, c不能用在范围之后，b断了 |
      | where a=3 and b like 'kk%' and c=4          | Y,使用到a,b,c                          |
      | where a=3 and b like '%kk' and c=4          | Y,使用到a                              |
      | where a=3 and b like '%kk%' and c=4         | Y,使用到a                              |
      | where a=3 and b like 'k%kk%' and c=4        | Y,使用到a,b,c                          |
      | where a>=5,<=9，>4,<10                      | Y，但是必须查询值5,9都存在表中         |
      | where a in (5,6,7)                          | Y, a                                   |
      | where a in (select id from t2 where c='cc') | N                                      |
      | where a between 5 and 9                     | Y，5,9都存在表中; N：5或9不在表中      |
      | **where a=3 or b=5**                        | **N**                                  |
      | where a !=3或a<>3                           | N                                      |

5.索引创建原则

6.索引类型

7.存储引擎区别

8.隔离级别

9.XGB 样本不平衡是否需要设置

- 用户需要提供一个和其它样本不同的值，然后把它作为一个参数传进去，

  ```javascript
  dtrain = xgb.DMatrix( data, label=label, missing = -999.0)
  ```

10.高斯混合模型和kmeans区别

- kmeans 属于hard分类，要么属于这个，要么属于 而GMM属于soft ，他是计算来自每个高斯分布概率，选择最大一个
- kmeans的决策边界是圆形，而GMM属于椭圆
- kmeans需要提前选择聚类中心，gmm需要提前设置每个聚类的初始传播（方差）设置为总体方差并相等地设置权重。
- GMM的准确度可能比kmeans更高
- 目标函数不同，kmeans是欧氏距离，而GMM是计算协方差的马氏距离

11.高斯算法

11.spark提交过程

12.缓存

13.repatition

14.is 和== 

- =代表数值相同，不管内存引用
- is 比较两个对象是不是完全相同，即内容相同，地址相同
- 但是python内置有个小的整数对象进行缓存，也就是所有创建的小整数变量直接从对象池进行引用 [-5,256]
- Cpython解释器会适当缓存一些常量和不包含特殊字符的字符串进行重用
- 如果包含可变变量，则就算值符合缓存器，内存也会改变，

